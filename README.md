# ğŸŒ OffGrid LLM

**AI for Edge & Offline Environments**

OffGrid LLM is a lightweight, offline-first LLM orchestrator designed for environments with limited or intermittent internet connectivity. Built in Go for maximum performance and minimal resource usage.

## ğŸ¯ Features

- âœ… **Offline-First**: Works without internet connectivity
- ğŸ”„ **P2P Model Sharing**: Share models across local networks
- ğŸ’¾ **USB Model Import**: Install models from USB drives/SD cards
- âš¡ **Low Resource**: Runs on devices with as little as 2GB RAM
- ğŸ”Œ **OpenAI-Compatible API**: Drop-in replacement for OpenAI API
- ğŸŒ **Edge-Ready**: Perfect for remote locations, ships, clinics, schools
- ğŸ“¦ **Single Binary**: Easy deployment, no dependencies

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/takuphilchan/offgrid-llm.git
cd offgrid-llm

# Quick start (builds and optionally downloads a model)
./scripts/quickstart.sh

# Or manually:
# Build
make build

# Run
./offgrid
```

Server will start on `http://localhost:8080`

See [Model Setup Guide](docs/MODEL_SETUP.md) for downloading models.

## ğŸ“š API Endpoints

```
GET  /health                  - Health check
GET  /v1/models              - List available models
POST /v1/chat/completions    - Chat completions (OpenAI-compatible, supports streaming)
POST /v1/completions         - Text completions (OpenAI-compatible)
```

### Streaming Support

Enable streaming by setting `"stream": true` in your request:

```bash
curl -N http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama-1.1b-chat",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
```

## ğŸ¯ CLI Commands

```bash
offgrid                          # Start server (default)
offgrid catalog                  # Browse available models
offgrid download <model-id>      # Download a model
offgrid list                     # List installed models
offgrid help                     # Show help
```

### Example: Download and Run

```bash
# Browse available models
offgrid catalog

# Download TinyLlama (638MB)
offgrid download tinyllama-1.1b-chat

# Start the server
offgrid

# Test in another terminal
curl http://localhost:8080/v1/models
```

## ğŸ—ï¸ Architecture

```
offgrid-llm/
â”œâ”€â”€ cmd/offgrid/           # Main application entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ server/           # HTTP server & API handlers
â”‚   â”œâ”€â”€ models/           # Model management & registry
â”‚   â”œâ”€â”€ inference/        # LLM inference engine
â”‚   â”œâ”€â”€ resource/         # Resource monitoring & allocation
â”‚   â””â”€â”€ p2p/              # Peer-to-peer networking
â”œâ”€â”€ pkg/api/              # Public API types
â””â”€â”€ web/ui/               # Web dashboard (future)
```

## ğŸ¯ Use Cases

- ğŸš¢ **Maritime & Offshore** - Ships, oil rigs, research vessels
- ğŸ¥ **Healthcare** - Rural clinics, mobile medical units
- ğŸ« **Education** - Schools in low-bandwidth areas
- ğŸ­ **Industrial** - Factories, mines, warehouses
- ğŸ”’ **High-Security** - Air-gapped networks
- ğŸ•ï¸ **Field Research** - Remote scientific operations

## ğŸ›£ï¸ Roadmap

### Phase 1 âœ… (Completed)
- [x] Basic HTTP server
- [x] OpenAI-compatible API structure
- [x] Model registry and management
- [x] Resource monitoring
- [x] Configuration system
- [x] P2P discovery foundation
- [x] Unit tests
- [x] **Streaming support (SSE)** â­ NEW
- [x] **P2P file transfer** â­ NEW

### Phase 2 (In Progress)
- [ ] llama.cpp integration
- [ ] Model loading from disk
- [ ] P2P model discovery & sharing (discovery done, integration pending)
- [ ] USB model import API
- [ ] Multi-user support
- [ ] Web dashboard

### Phase 3
- [ ] Advanced quantization
- [ ] Bandwidth-aware syncing
- [ ] Web dashboard
- [ ] Mobile/ARM optimization
- [ ] Docker support

## ğŸ“– Documentation

- [Model Setup Guide](docs/MODEL_SETUP.md) - Download and configure models
- [Architecture & Distribution Strategy](docs/ARCHITECTURE.md) - How offline distribution works
- [API Documentation](docs/API.md) - Complete API reference
- [Quick Start Script](scripts/quickstart.sh) - Automated setup

## ğŸ”§ Scripts & Tools

```bash
./scripts/quickstart.sh              # Interactive setup with model download
./scripts/create-usb-package.sh      # Create offline USB installation
./scripts/example_client.sh          # Bash API examples
./scripts/example_client.py          # Python API examples
```

## ğŸ’¾ Offline Distribution

### USB Package Creation

```bash
# Create complete offline package
./scripts/create-usb-package.sh /media/usb tinyllama-1.1b-chat

# Result: USB drive with binaries, models, docs, installers
# Works on Linux, Windows, macOS - no internet needed!
```

### Model Catalog

Built-in catalog with 4 recommended models:
- **TinyLlama 1.1B** - Lightweight (638MB, 2GB RAM)
- **Llama 2 7B** - Balanced quality (3.8GB, 8GB RAM) 
- **Mistral 7B** - Excellent for code (4.1GB, 8GB RAM)
- **Phi-2** - Efficient reasoning (1.5GB, 4GB RAM)

## ğŸ§ª Testing

```bash
# Run tests
make test

# Build
make build

# Run in development mode
make dev
```

## ğŸ¤ Contributing

Contributions welcome! This project aims to make AI accessible in underserved environments.

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ’¡ Philosophy

**AI should work everywhere, not just where the internet is fast.**

OffGrid LLM brings powerful language models to edge environments, remote locations, and anywhere reliable internet connectivity isn't guaranteed.
