# OffGrid LLM - GPU Docker Image (NVIDIA CUDA)
# Build from project root: docker build -f docker/Dockerfile.gpu -t offgrid-llm:gpu .

# =============================================================================
# Stage 1: Build the Go binary
# =============================================================================
FROM golang:1.21-bookworm AS go-builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    git \
    make \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /build

# Copy go mod files first for caching
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY cmd/ ./cmd/
COPY internal/ ./internal/
COPY pkg/ ./pkg/
COPY web/ ./web/

# Build the application
RUN CGO_ENABLED=1 GOOS=linux go build \
    -a -ldflags '-extldflags "-static"' \
    -o offgrid ./cmd/offgrid/

# =============================================================================
# Stage 2: Build llama.cpp with CUDA support
# =============================================================================
FROM nvidia/cuda:12.2.0-devel-ubuntu22.04 AS llama-builder

RUN apt-get update && apt-get install -y \
    git \
    cmake \
    make \
    g++ \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /build

# Clone and build llama.cpp with CUDA
RUN git clone --depth 1 https://github.com/ggerganov/llama.cpp.git && \
    cd llama.cpp && \
    cmake -B build \
        -DGGML_CUDA=ON \
        -DLLAMA_BUILD_SERVER=ON \
        -DCMAKE_CUDA_ARCHITECTURES="60;70;75;80;86;89;90" && \
    cmake --build build --config Release -j$(nproc) --target llama-server && \
    strip build/bin/llama-server

# =============================================================================
# Stage 3: Final runtime image with CUDA
# =============================================================================
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -g 1000 offgrid && \
    useradd -u 1000 -g offgrid -m offgrid

# Create required directories
RUN mkdir -p /var/lib/offgrid/models \
             /var/lib/offgrid/web/ui \
             /var/lib/offgrid/data \
             /var/lib/offgrid/bin && \
    chown -R offgrid:offgrid /var/lib/offgrid

# Copy binaries from builders
COPY --from=go-builder /build/offgrid /usr/local/bin/offgrid
COPY --from=llama-builder /build/llama.cpp/build/bin/llama-server /var/lib/offgrid/bin/llama-server

RUN chmod +x /usr/local/bin/offgrid /var/lib/offgrid/bin/llama-server

# Copy web UI
COPY --from=go-builder /build/web/ui /var/lib/offgrid/web/ui

# Switch to non-root user
USER offgrid
WORKDIR /var/lib/offgrid

# Expose default port
EXPOSE 11611

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3 \
    CMD curl -f http://localhost:11611/health || exit 1

# Environment variables
ENV OFFGRID_MODELS_DIR=/var/lib/offgrid/models \
    OFFGRID_DATA_DIR=/var/lib/offgrid/data \
    OFFGRID_BIN_DIR=/var/lib/offgrid/bin \
    OFFGRID_PORT=11611 \
    OFFGRID_HOST=0.0.0.0 \
    OFFGRID_GPU_LAYERS=99

# Run the server
CMD ["/usr/local/bin/offgrid"]
