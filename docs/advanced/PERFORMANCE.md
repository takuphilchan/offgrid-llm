# Performance Optimization Guide

## Fast Model Loading

OffGrid LLM now includes optimizations for **significantly faster startup and first response times**, especially on low-end hardware.

### Automatic Optimizations

When you run `offgrid run <model>`, the llama-server is automatically started with these performance flags:

#### Adaptive Memory Mode
- **Low RAM (<8GB)**: Uses memory-mapped files (mmap) to prevent crashes
- **High RAM (â‰¥8GB)**: Loads model directly into RAM with mlock for speed
- **Automatic detection**: System RAM is checked at startup

#### Flash Attention (-fa)
- Enables optimized attention mechanism
- **Speed improvement**: 20-40% faster inference
- Lower memory usage during inference

#### Continuous Batching (--cont-batching)
- Better throughput for multiple requests
- Reduces latency when handling concurrent requests

#### Quantized KV Cache (--cache-type-k q8_0 --cache-type-v q8_0)
- Uses INT8 quantization for key-value cache
- **Memory savings**: ~50% less cache memory
- Minimal quality loss (imperceptible in most cases)

#### Prompt Caching (--cache-prompt)
- **NEW**: Reuses computed prompt tokens across requests
- **Speed improvement**: 2-5x faster for multi-turn conversations
- Critical for chat applications where system prompts repeat

#### Lower Batch Size (-b 256)
- Reduces latency for first token generation
- Better for interactive chat on low-end hardware

#### Adaptive Context Size
- Context window automatically scales based on available RAM:
  - <4GB RAM: 1024 tokens
  - 4-6GB RAM: 2048 tokens
  - 6-12GB RAM: 4096 tokens
  - 12GB+ RAM: 8192 tokens

#### Optimal Thread Configuration
- Automatically uses physical cores (not hyperthreads)
- Leaves 1 core for OS operations
- No manual configuration needed

### Performance Comparison

| Configuration | First Response | Subsequent Responses | RAM Usage |
|--------------|----------------|---------------------|-----------|
| **Old Defaults** | 8-15 seconds | 0.5-2 seconds | 4-6 GB |
| **New Optimized** | **2-4 seconds** | **0.2-0.8 seconds** | 4-8 GB |
| **Low RAM Mode** | 3-6 seconds | 0.4-1.2 seconds | 3-4 GB |

### Manual Configuration

If you need to customize these settings:

1. **Direct startup**: Edit `/cmd/offgrid/main.go` in `startLlamaServerInBackground()`
2. **System service**: Edit `/usr/local/bin/llama-server-start.sh`
3. **Environment variables** (see below)

### Environment Variables

```bash
# Performance tuning
export OFFGRID_BATCH_SIZE=256          # Token batch size (lower = faster first token)
export OFFGRID_FLASH_ATTENTION=true    # Enable flash attention
export OFFGRID_KV_CACHE_TYPE=q8_0      # KV cache quantization: f16, q8_0, q4_0
export OFFGRID_USE_MMAP=true           # Memory-map model (good for low RAM)
export OFFGRID_USE_MLOCK=false         # Lock model in RAM (only if RAM >= model size)
export OFFGRID_CONT_BATCHING=true      # Continuous batching
export OFFGRID_LOW_MEMORY=true         # Enable all low-memory optimizations
export OFFGRID_ADAPTIVE_CONTEXT=true   # Auto-adjust context based on RAM
```

### Low-End Hardware Tips

For systems with **4GB RAM or less**:

1. **Use 1B-3B models**: 
   ```bash
   offgrid search llama --ram 4
   offgrid download-hf bartowski/Llama-3.2-1B-Instruct-GGUF
   ```

2. **Use aggressive quantization**: Q3_K_M or Q4_K_S

3. **Reduce context size**:
   ```bash
   export OFFGRID_MAX_CONTEXT=1024
   ```

4. **Enable low memory mode**:
   ```bash
   export OFFGRID_LOW_MEMORY=true
   ```

### Advanced Optimizations

#### Keep llama-server Running (Daemon Mode)

For the **absolute fastest** experience, keep llama-server running persistently:

```bash
# Enable the systemd service to start on boot
sudo systemctl enable llama-server

# Start it now
sudo systemctl start llama-server

# Check status
sudo systemctl status llama-server
```

**Benefits**:
- Zero startup time
- Model stays loaded in RAM
- Instant responses (< 500ms for first token)

#### GPU Acceleration

If you have a CUDA-capable GPU:

```bash
# Rebuild llama.cpp with GPU support
cd /tmp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make LLAMA_CUBLAS=1

# Install the GPU-enabled binary
sudo cp llama-server /usr/local/bin/

# Restart the service
sudo systemctl restart llama-server
```

**Speed improvement**: 5-20x faster inference depending on GPU

#### Model Quantization

Use smaller quantizations for faster loading:

| Quantization | Model Size | Load Time | Quality |
|-------------|-----------|-----------|---------|
| Q4_K_M | ~2.5 GB | 1-2 sec | [Star][Star][Star][Star][ ] (Recommended) |
| Q5_K_M | ~3.0 GB | 2-3 sec | [Star][Star][Star][Star][Star] |
| Q8_0 | ~5.0 GB | 4-6 sec | [Star][Star][Star][Star][Star] |
| F16 | ~7.0 GB | 6-10 sec | [Star][Star][Star][Star][Star] |

**Recommendation**: Use `Q4_K_M` for the best balance of speed and quality.

### Troubleshooting

#### "Out of Memory" Errors

If you get OOM errors with `--no-mmap --mlock`:

1. **Use mmap mode** (slower first response, less RAM):
   ```bash
   # Edit the startup command to remove --no-mmap --mlock
   # Use mmap instead (automatic in llama-server)
   ```

2. **Use a smaller model**:
   ```bash
   # Download a smaller quantization
   offgrid download-hf <model-id> --quant Q4_K_S
   ```

3. **Increase swap space** (not recommended for performance):
   ```bash
   sudo fallocate -l 8G /swapfile
   sudo chmod 600 /swapfile
   sudo mkswap /swapfile
   sudo swapon /swapfile
   ```

#### First Response Still Slow

Check if model is actually loaded:

```bash
# Check llama-server logs
sudo journalctl -u llama-server -f

# Test the endpoint
curl http://localhost:<llama-port>/health
```

If "status" shows "loading model", wait for it to complete. Large models (>10GB) can take 30-60 seconds.

### Benchmarking

Test your model's performance:

```bash
# Run built-in benchmark
offgrid benchmark <model-name>

# Test end-to-end latency
time offgrid run <model-name> <<< "Hello, how are you?"
```

## Summary

**For fastest experience**:
1. Use Q4_K_M quantization (best speed/quality balance)
2. Enable systemd service for persistent llama-server
3. Use optimized flags (automatic in offgrid v0.1.6+)
4. Add GPU support if available
5. Ensure enough RAM for --mlock

**Current defaults provide**:
- 2-4 second first response (vs 8-15 seconds before)
- <1 second subsequent responses
- Minimal quality loss with optimized caching
